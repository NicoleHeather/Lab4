**SENG 438 - Software Testing, Reliability, and Quality**

**Lab. Report \#4 – Mutation Testing and Web app testing**

| Group 23:      |     |
| -------------- | --- |
| Nicole Heather |     |
| Nora Mellik    |     |
| Jennifer Jay   |     |
| Creek Thompson |     |

# Introduction
As software project becomes larger in scale and more logicically complicated, detecting errors and mistakes in code becomes increasingly difficult. Requirement and coverage based testing does well to examine a programs ability to function correctly, but if a tester does not know the possible faults in the system, how do they detect them? Mutation testing is the perfect tool to cover for the faults of the previous testing strategies. 

Part 1: Mutation Testing

Firstly, we'll explore Mutation Testing, a technique that allows us to evaluate the effectiveness of our test suites. By injecting faults, or mutations, into the source code and analyzing how well our tests detect these faults, we can gauge the quality of our testing efforts. Using Pitest, a powerful mutation testing tool, we'll generate mutants of the System Under Test (SUT) and measure mutation scores to improve our test suite's overall robustness.

Objectives:
- Gain an understanding of mutation testing and its significance.
- Measure mutation scores to assess test suite effectiveness.
- Familiarize ourselves with Pitest and its functionalities.

Part 2: GUI Testing

Next, we'll venture into the realm of GUI Testing, specifically focusing on automated testing. Our primary tool for this will be Selenium, a widely-used web-interface testing tool. We'll automate test cases on selected websites to ensure their functionality and reliability. Additionally, we'll explore an alternative tool, Sikulix, comparing its features and capabilities with Selenium.

Objectives:
- Familiarize ourselves with Selenium and Sikulix for automated GUI testing.
- Design and automate UI test cases for selected functionalities of target applications.
- Compare Selenium IDE with Sikulix, discussing their pros and cons.

Throughout both parts of the lab, meticulous documentation and analysis will be key. We'll record our findings, discuss strategies for improving mutation scores, evaluate the advantages and disadvantages of testing tools, and reflect on our teamwork dynamics.

# Analysis of 10 Mutants of the Range class 

5 Mutants Killed:
- #105: Replaced double return with 0.0d for getLowerBound():
        How this mutation was killed: 
        GetLowerBound() is a function that only has one line of code that returns the lower field value of class Range. Because this function is so simple, the test cases developed for it always knew the specified output and if that returned value ever changed from what it was set as in the range constructor then the test will fail.

- #132: Replaced double addition with division for getCentralValue():
        How this mutation was killed: 
        Similarly to the test mutation above, there is only one instruction in getCentralValue(). However, uniquely the logic is more complex which leads to vulnerabilities to mutations. When writting the these test cases, we knew the outputs we wanted and if the calculated value was not what was given in the assertion it would always fail. This mutaiton was also killed through the use of the variety of input variables, meaning that we covered as many possible inputs and outputs as possible. 

- #144: Greater than to equal for contains(double value):
        How this mutation was killed:
        Again, contains() has only a single line of logic, but that logic is more complex than just a return value. We can identify the test that had killed this as the test case contains_EqualsLower, as this function already looks for a value that is either equal to or grater than lower in order to see if it contains said value.

- #123: Negatated double field lower for getLength():
        How this mutation was killed:
        The value of length was always known during the creation of the test cases, along with the range of different inputs used for testing resulted in the killing of this mutation. There where some possible cases that would have let the mutation survive, such as setting lower to 0 as that would technically be negation, but we had multiple other range values that kept this mutation in check. This includes input ranges such as setting upper and lower equal, or as opposite ranged values.

- #161: Less than to greater or equal for intersects(double b0, double b1):;
        How this mutation was killed:
        Cases for this function include test such as testIntersectsB1BiggerThanLowerBound(), testIntersectsB0LowerThanUpperAndB1isEqualB0() and testIntersectsB0LowerThanUpperAndB1isLessThanB0(). Combined with the many other cases in this particular class, we where able to account for the many different possible variance in the logic of the intersect function.

Overall when it comes to the killing of mutants for class Range, our bests asset in our current test cases is the variety of inputs and scenarioes test for. This help greatly with the catching of mutations as we value the consistancy of our methods greatly.

5 Mutants Not Killed: 
- #144: Negated double field lower for contains(double value):
        How this mutation survived:
        This mutation survived due to the way we originally set up the Range object we used for testing, we never changed the values of lower or upper to anything other than positive values. Resulting in a multitude of mutations that survived due to a lack of variety in the parameter of the class. To fix this we would simply need to add new test cases that change the value set in the Range Object.

- #190: Changed conditional boundary for constrain(double value): 
        How this mutation survived:
        This mutation survived partly due to how the logic of the function operates, as well as how the test cases where made. Much like the test mutation above, we lacked any variety in the values that the Range object had and therefore could not take into account a variety of different mutations. The logic of this function also depends on the output of another function, and if we are only injecting mutations into one method it can be hard to keep track of the different paths a single mutation an make.

- #271: equal to greater or equal for min(double d1, double d2):
        How this mutation survived:
        This is a case of both a equivalent mutation and a difference in method logic. The equaivalent mutation here is the changing of a true or equal value to a equal or greater than value, this mutation does not change the behaviour of the program as this specific branch will still always be taken if Nan is true. Following that, Nan cannot be compared to see if something is greater or less than, it is simply true or false, as a result this mutation was not killed by any of our test cases. 
        
- #281: removed call to java/lang/Double::isNan for max(double d1, double d2):
        How this mutation survived:
        This mutation survived due to no test class existing specifically for max. Previously we only tested the line, branch and method coverage of max in which this method was called by another method. This means that we had not actually tested the possible scenarios with this function outside of the one that was calling it which is not effective. In order to fix this we would need to create a test case that specifically focuses on the output of this function and not how it is used in another one.

- #463: Substituted 32 with -32 for hashCode(): 
        How this mutation survived:
        HashCode() is responsible for the transformation of upper and lower into hashes. Since 32 is just the value of bits to shift, changing it to -32 does not actually effect the output. This is to say that this is essentiallly an equivalent mutation and as such it cannot be  killed. This does effect the possible coverage we can get, so we have to make up for it by killing as many other mutants as possible.

# Report all the statistics and the mutation score for each test class

Original Test Coverage for Assignment 3 Test Classes:

![alt text](<Report Screenshots/OG_mutation_coverage_C.png>)

After the Creation of MutationTest.java:

![alt text](<Report Screenshots/New_mutation_Coverage_C.png>)

Please Note: Due to the constrains on the source code, a mutlitude of equivalent classes and the original mutation coverage for DataUtilities being ratehr high, we where unable to increase the coverage any further. Also note that these metrics do not include the test given to us in the assignment artifact as the ywould inflate the score of classes we are trying to test with our original suite.

# Analysis drawn on the effectiveness of each of the test classes

The test class that was by far the most efficient was DataUtilitiesTest, this is due to a number of factors. Starting with the amount of functions in the source class, although sometimes more logically complex than the methods in Range the lower quantity results in mutations becoming a little eaiser to single out. It is also worth noting that along with some of the unreachible code that cannot be covered and as such their mutations as well, some injected mutations dont entrely relfect the proper use or functionality of these methods, or we might have numerous equivalent mutants that cannot be killed. As a result the mutation coverage score reached a threshold in which it cannot be improved further. 

The testing classes that where the least effective where the test classes that belonged to the Range source file (CombineTest, ShiftTest, RangeEqualsTest, etc). This is due to a number of factors, partly mentioned in the mutation report above, another being the sheer amount of methods that need to be both line, branch and method covered but also cover as many possible mutations as possible. This lack of coverage may also be a result of the amount of unreachible code along with equivalent mutations that are present in the source class. However, this lack of coverage does have it advantages, we as test developers have more knowledge on how we can improve our current test classes.

# A discussion on the effect of equivalent mutants on mutation score accuracy

Automatic Detection of Equivalent Mutants in Experimentation:

Approach: Semantic Analysis for Equivalent Mutant Detection

Methodology:
1. Semantic Comparison: Perform a semantic comparison between the original code and each mutant to identify if they exhibit equivalent behavior.
  
2. Execution-based Analysis: Execute test cases against both the original code and mutants to observe their outputs and behaviors. Compare the outputs of mutants with the original code to detect equivalence.

3. Symbolic Execution: Use symbolic execution to explore all possible execution paths for both the original code and mutants. Compare the execution paths and their outcomes to identify equivalent mutants.

4. Invariant Analysis: Analyze invariants present in the code and mutants to detect if they maintain the same program properties across executions.

Benefits:
- Automated Detection: The approach automates the process of detecting equivalent mutants, reducing the manual effort required for experimentation.
  
- Comprehensive Analysis: By considering semantic aspects and execution behaviors, the approach provides a comprehensive understanding of mutant equivalence.

- Enhanced Mutation Testing: Detecting equivalent mutants improves the efficiency of mutation testing by reducing redundant mutants and focusing on those that contribute to test effectiveness.

Disadvantages:
- Complexity: Implementing semantic analysis techniques and symbolic execution tools can be complex and resource-intensive.

- False Positives/Negatives: Semantic analysis may sometimes fail to capture all nuances of code behavior, leading to false positives or negatives in mutant equivalence detection.

- Overhead: Execution-based analysis and symbolic execution can introduce overhead in terms of computational resources and time.

Assumptions:
- Semantic Equivalence: The approach assumes that mutants exhibiting the same behavior as the original code are functionally equivalent, which may not always hold true in all contexts.

- Test Coverage: The effectiveness of the approach relies on comprehensive test suites that adequately cover the code and its mutants.

- Soundness and Completeness: The analysis techniques used should aim for both soundness (no false positives) and completeness (no false negatives) to ensure accurate detection of equivalent mutants.

In conclusion, automatic detection of equivalent mutants through semantic analysis presents a promising approach to enhance mutation testing efficiency. However, it requires careful consideration of complexities, potential inaccuracies, and the balance between automation and manual intervention. Continued research in this area could lead to more effective techniques for identifying equivalent mutants and improving the reliability of mutation testing.

Manual Equivilant Muntant Detection
In the manual equivilant muntant testing, we looked at the mutation logs generated by the tool. We looked through the list of mutations to determine if any of them had the potenial to be classified as equlivant, this included looking at the details of both the mutant and the source code. In DataUtlities, we noticed many "removed call to org/jfree/chart/util/ParamChecks::nullNotPermitted" which result in the same behavior as the source code. There were also two negated conditional mutations in the equal function which also resulted in the same behavior as the orginal code. Equivalent mutants do not affect the mutation coverage score.Equivalent mutants are mutations that, despite being different from the original code, behave identically to it in terms of the test outcomes. Therefore, when a mutation is equivalent, it doesn't contribute to increasing the number of detected mutations by the test suite. As a result, the mutation coverage score remains unaffected.

# A discussion of what could have been done to improve the mutation score of the test suites

Through the use of Pitest and the close examination of it reports, we found specific areas in which we were lacking test coverage. We also did our best to identify cases in which testing them would kill multiple mutations, as that would make our test cases not only effective but efficient as Pitest remains to be computationally and time exhaustive. 

The test classes for Range had the biggest room for improvement. The best ways to accomplish that are to expand on the original example range used for the tests in each case, this would greatly improve our chances of catching mutations that change around the behaviour of branches, computations and return values. For many of our original test cases, we only worked with values that where legal and within the requirements set by the constructor. However for functions that rely on basic input variables (max(), min(), constrain(), etc) and not upper and lower, we have far more freedom to cover as many different scenarios as possible.

As for DataUtilities, this specific class already had its mutation coverage very high so it was very difficult to identify further improvements to our test suite. The few improvements we did do included the focus on the catching of exceptions and further exploration of branches or sections of code not tested thoughouly before. This did help our coverage somewhat, but with the existance of equivalent mutations and the small amount of methods used the increase in coverage was small.

# Why do we need mutation testing? Advantages and disadvantages of mutation testing

We need mutation testing, not specifically to test the source code itself but the strengths and weakensses of our own test code. Large programs with an abundance of logic, methods and classes becomes harder and harder to manually to test as the man-power, cost and time to make them increases. Development sometimes also requires small chnages, additions or subractions from exitsing code, we need to be able to design code that not only tests the correctness and requirements of the code but also code that accuratley reflects the behaviour of these methods. The injection of mutaitons themselves are changes to the method behaviour, this ensures that the test developer actually understands the context in which these functions must function appropriatley in. 

Advantages: 
- The biggest advantage to mutation testing is its ability to be automized, the test developer does not have to manually change the source code to ensure the strenght of their test suites.
- Mutation testing is most effective at the unit testing level, this includes both the design and evaluation of test cases. This is due to its focus on individual behaviours for each test and their corresponding methods and classes.
- Reporting both evaluation metrics and the mutation process is increadibly useful to test developers. They provide targets for developers to focus on, and the percentage calculations provided also give testing thresholds for people to decide on where they want to exit the testing phase. 
- This testing strategy is rather methotical, is does well with having a plan or system to follow. In which is very common in the testing phase of development. 

Disadvantages: 
- Due to its automated nature, it is very computationallly expensive. Even smaller programs, like this lab (In which we have tests for only 2 classes) it takes quite a bit of time and power to proccess. This also means this style of testing does not scale very well as a result. 
- Sometimes, mutations that are injected into code may not be indicative of actual faults with either the source code or test cases.
- It is entirely possible for equivalent mutants to lower mutation coverage due to their nature as being unkillible. In order to counteract this, developers need to understand wheree theur equivalent mutants lie. 

# Explain your SELENUIM test case design process

In order to design our tests, we initially brainstormed what interactive elements of the eBay website could be consistently recorded and tested. The final tests were based on browsing the site ourselves and jotting down relevant functionalities that could be tested, with elements that existed that could be asserted once done. At the end of the brainstorming and traversing the website, the eight tests we came up with were Add to Cart, Sign In, Remove from Cart, Add to Watchlist, Save Seller, Invalid Login, Normal Search, and Advanced Search. Creating the test cases involved using Selenium to record the test case, which was then saved into a test case within the project file and would be run several times to ensure the functionality was working as expected.

# Explain the use of assertions and checkpoints

Each test has an assert integrated at the end of the recording through the provided utility to create assertions in Selenium. Most tests use a text-based assertion, comparing an expected text value to where it should be in the recorded testing process. The tests which use a text assertion are Add to Cart (checks if the name of the correct item has been placed in cart), Add to Watchlist (checks if the name of the correct item has been placed in the watchlist), Invalid Account Name (checks if error message text pops up), Remove from Cart (checks if empty cart text is present once the item is removed), Sign In (checks if the username text is present in the corner), Normal Search (checks the text of the name of the item being searched), and Advanced Search (two asserts, one which checks for correct text of item keywords and a second for checking if the applied filter text appeared). The Save Seller test uses an assertion that checks if an element is present rather than comparing text.

# how did you test each functionaity with different test data

Test: “add to cart” 
This was tested by going to the link of a specific item, adding it to the cart, then going into the cart to look at the item. The value checked is if the name of the item is in the cart, “Bose QuietComfort 45 Noise Cancelling Headphones, Certified Refurbished”. There is an assert for the text.

Test: “add to watchlist”
This test must be run while signed in. This feature was tested by going to the link of a specific item, adding it to the watchlist, going to the watchlist, then checking if the name of the item is in the watchlist. The value checked is if the name of the item is in the watchlist, “Bose Portable Home Speaker Bluetooth, Certified Refurbished”. There is an assert for the text.

Test: “invalid account name”
This test must be run while signed out. This feature was tested by putting in a username (“gggg”) for an account that does not exist. The value checked was if the error message for incorrect username appears. This uses a text assert for the message “We couldn't find this eBay account.”.

Test: “remove from cart”
This test must be run when something is in the cart. This feature was tested going into the cart and clicking the Remove button. The value checked was if the cart is now empty through the appearance of a message to the user. This uses a text assert for the message “You don't have any items in your cart. Let's get shopping!”.

Test: “save seller”
This test must be run while signed in. This feature was tested by going to a specific item’s seller, saving the seller, then checking the Saved page. The value being checked was if the seller’s shop appears in the Saved page. This uses an element present assert for the seller’s square that appears in the Saved page.

Test: “sign in”
This test must be run while signed out. This feature is tested by inputting a user’s email and password, resulting in a successful sign in. The value being checked is if the username appears in the corner next to “Hi”. There is a text assertion for the username “EBay”. A note regarding this test is occasionally a CAPTCHA will appear during sign in due to Ebay’s robot detection, which the test does not accommodate for, as this only appears once every Ebay session, meaning running the test once the CAPTCHA has been done manually will ensure a passing test.

Test: “normal search”
This was tested by searching for a specific listing in the search bar. The value used for the test data is “headphones”. Assert text was used to check that the value is shown in the results.

Test: “advanced search”
This was tested by using the Advanced Search page. The test data included options for “Enter keywords or item number” (value=Hot Wheels 2024), “Keyword options” (label=Exact words, any order), and “Show Results” where the “Free Returns” checkbox was clicked. After searching, the assert text was used to check that the “Keyword options” is used (value="Hot" "Wheels" "2024") where each word from the value used for “Enter keywords or item number” is in its own quotation marks. Assert text was also used to ensure that  the “Free Returns” filter is displayed (value=Free Returns).

# Discuss advantages and disadvantages of Selenium vs. Sikulix

Sikulix vs Selenium
Sikuli is used for GUI test automation and identifies objects using that are displayed on the screen. It requires an image to be stored and uses an image-based recognition system to compare how much the image matches the display GUI it is searching. This can be particularly helpful for test cases such as searching on Ebay as it can also recognize the images of the search results.
A disadvantage of Selenium is that it can only be used on web applications where as Sikulix can also be used on system applications as well. Selenium also cannot be used for image recognition and is only limited to locators that can be found in the HTML source code, where as Sikulix is not limited to the HTML source code as it compares images to anything that can be seen on screen. A disadvantage of Sikulix is that it can work only with what is displayed on the GUI. Unlike Selenium, Sikulix is unable to read text and images must be stored to test automation.

# How the team work/effort was divided and managed

To start off with, all members where present when planning the work and distribution for this lab, each member was also assigned to where we thought their strengths would apply the most. Due to time constaints, we had a relativley strict schedule to follow before the demo day. Team work was divided into pairs, two working on the mutation testing using Pitest and the other took care of Selenium with the GUI testing. This did not mean that these pairs did not have any knowledge of the testing they did not do, proper steps were taken to ensure that all members understood the steps taken to complete each lab section. 

# Difficulties encountered, challenges overcome, and lessons learned

The biggest issues with this lab where the setting up of Pitest. In particular there were problems with the specific jdk used for Pitest as some versions did support it, but Pitest failed to run due to class errors, this extends to a bigger issue of sometimes there not being enough instructions to set up projects. Some if the lab instructions themselves mentioned folders that did not exist or where missing some external jar files for library purposes. It had taken some time to get the logistics working but after we did the execution went smoothly. 

In regards to the lessons leanred from this lab, despite its setbacks, this was a insighful look into the use and practice of both mutation and GUI testing. Leanring how to use these tools are very benificial, we undertsand them now in order to use in industry but students now have another way to implement quality control in larger projects if needed.

# Comments/feedback on the lab itself

Much like the previous lab, some points of the code in both Range and DataUtilities where unreachible, which in turn hurt of mutation scores. Also, the act of running the test files within the assignment artifacts was rather time and computing expensive, a smaller sample to test Pitest might have been fine. However the tools we used in this lab where very intersting, especially seeing how Selenuim works with its focus on GUI testing. 
