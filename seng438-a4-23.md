**SENG 438 - Software Testing, Reliability, and Quality**

**Lab. Report \#4 â€“ Mutation Testing and Web app testing**

| Group 23:      |     |
| -------------- | --- |
| Nicole Heather |     |
| Nora Mellik    |     |
| Jennifer Jay   |     |
| Creek Thompson |     |

# Introduction
As software project becomes larger in scale and more logicically complicated, detecting errors and mistakes in code becomes increasingly difficult. Requirement and coverage based testing does well to examine a programs ability to function correctly, but if a tester does not know the possible faults in the system, how do they detect them? Mutation testing is the perfect tool to cover for the faults of the previous testing strategies. 

Part 1: Mutation Testing

Firstly, we'll explore Mutation Testing, a technique that allows us to evaluate the effectiveness of our test suites. By injecting faults, or mutations, into the source code and analyzing how well our tests detect these faults, we can gauge the quality of our testing efforts. Using Pitest, a powerful mutation testing tool, we'll generate mutants of the System Under Test (SUT) and measure mutation scores to improve our test suite's overall robustness.

Objectives:
- Gain an understanding of mutation testing and its significance.
- Measure mutation scores to assess test suite effectiveness.
- Familiarize ourselves with Pitest and its functionalities.

Part 2: GUI Testing

Next, we'll venture into the realm of GUI Testing, specifically focusing on automated testing. Our primary tool for this will be Selenium, a widely-used web-interface testing tool. We'll automate test cases on selected websites to ensure their functionality and reliability. Additionally, we'll explore an alternative tool, Sikulix, comparing its features and capabilities with Selenium.

Objectives:
- Familiarize ourselves with Selenium and Sikulix for automated GUI testing.
- Design and automate UI test cases for selected functionalities of target applications.
- Compare Selenium IDE with Sikulix, discussing their pros and cons.

Throughout both parts of the lab, meticulous documentation and analysis will be key. We'll record our findings, discuss strategies for improving mutation scores, evaluate the advantages and disadvantages of testing tools, and reflect on our teamwork dynamics.

# Analysis of 10 Mutants of the Range class 
5 Mutants Killed:

- #105: Replaced double return with 0.0d for getLowerBound():
        How this mutation was killed: getLowerBound() is a function that only has one line of code that returns the lower field value of class Range. Because this function is so simple, the test cases developed for it always knew the specified output and if that returned value ever changed from what it was set as in the range constructor then the test will fail.

- #132: Replaced double addition with division for getCentralValue():
        How this mutation was killed: Similarly to the test mutation above, there is only one instruction in getCentralValue(). However, uniquely the logic is more complex which leads to vulnerabilities to mutations. When writting the these test cases, we knew the outputs we wanted and if the calculated value was not what was given in the assertion it would always fail. This mutaiton was also killed through the use of the variety of input variables, meaning that we covered as many possible inputs and outputs as possible. 

- #144: Greater than to equal for contains(double value):
        How this mutation was killed: Again, contains() has only a single line of logic, but that logic is more complex than just a return value. We can identify the test that had killed this as the test case contains_EqualsLower, as this function already looks for a value that is either equal to or grater than lower in order to see if it contains said value.

- #123: Negatated double field lower for getLength():
        How this mutation was killed:

- #161: Negated double field upper for intersects(double b0, double b1):;
        How this mutation was killed:

5 Mutants Not Killed: 
- #123: Incremented (a++) double field upper for getLength():
- #190: Less or equal to less than for constrain(): 
- #271: equal to greater or equal for min():
- #281: removed call to java/lang/Double::isNan for max():
- #463: Substituted 32 with -32 for hashCode(): 
    HashCode() is responsible for   


# Report all the statistics and the mutation score for each test class


# Analysis drawn on the effectiveness of each of the test classes

# A discussion on the effect of equivalent mutants on mutation score accuracy

Automatic Detection of Equivalent Mutants in Experimentation:

Approach: Semantic Analysis for Equivalent Mutant Detection

Methodology:
1. Semantic Comparison: Perform a semantic comparison between the original code and each mutant to identify if they exhibit equivalent behavior.
  
2. Execution-based Analysis: Execute test cases against both the original code and mutants to observe their outputs and behaviors. Compare the outputs of mutants with the original code to detect equivalence.

3. Symbolic Execution: Use symbolic execution to explore all possible execution paths for both the original code and mutants. Compare the execution paths and their outcomes to identify equivalent mutants.

4. Invariant Analysis: Analyze invariants present in the code and mutants to detect if they maintain the same program properties across executions.

Benefits:
- Automated Detection: The approach automates the process of detecting equivalent mutants, reducing the manual effort required for experimentation.
  
- Comprehensive Analysis: By considering semantic aspects and execution behaviors, the approach provides a comprehensive understanding of mutant equivalence.

- Enhanced Mutation Testing: Detecting equivalent mutants improves the efficiency of mutation testing by reducing redundant mutants and focusing on those that contribute to test effectiveness.

Disadvantages:
- Complexity: Implementing semantic analysis techniques and symbolic execution tools can be complex and resource-intensive.

- False Positives/Negatives: Semantic analysis may sometimes fail to capture all nuances of code behavior, leading to false positives or negatives in mutant equivalence detection.

- Overhead: Execution-based analysis and symbolic execution can introduce overhead in terms of computational resources and time.

Assumptions:
- Semantic Equivalence: The approach assumes that mutants exhibiting the same behavior as the original code are functionally equivalent, which may not always hold true in all contexts.

- Test Coverage: The effectiveness of the approach relies on comprehensive test suites that adequately cover the code and its mutants.

- Soundness and Completeness: The analysis techniques used should aim for both soundness (no false positives) and completeness (no false negatives) to ensure accurate detection of equivalent mutants.

In conclusion, automatic detection of equivalent mutants through semantic analysis presents a promising approach to enhance mutation testing efficiency. However, it requires careful consideration of complexities, potential inaccuracies, and the balance between automation and manual intervention. Continued research in this area could lead to more effective techniques for identifying equivalent mutants and improving the reliability of mutation testing.

Manual Equivilant Muntant Detection
In the manual equivilant muntant testing, we looked at the mutation logs generated by the tool. We looked through the list of mutations to determine if any of them had the potenial to be classified as equlivant, this included looking at the details of both the mutant and the source code. In DataUtlities, we noticed many "removed call to org/jfree/chart/util/ParamChecks::nullNotPermitted" which result in the same behavior as the source code. There were also two negated conditional mutations in the equal function which also resulted in the same behavior as the orginal code. Equivalent mutants do not affect the mutation coverage score.Equivalent mutants are mutations that, despite being different from the original code, behave identically to it in terms of the test outcomes. Therefore, when a mutation is equivalent, it doesn't contribute to increasing the number of detected mutations by the test suite. As a result, the mutation coverage score remains unaffected.

# A discussion of what could have been done to improve the mutation score of the test suites

# Why do we need mutation testing? Advantages and disadvantages of mutation testing

We need mutation testing, not specifically to test the source code itself but the strengths and weakensses of our own test code. Large programs with an abundance of logic, methods and classes becomes harder and harder to manually to test as the man-power, cost and time to make them increases. Development sometimes also requires small chnages, additions or subractions from exitsing code, we need to be able to design code that not only tests the correctness and requirements of the code but also code that accuratley reflects the behaviour of these methods. The injection of mutaitons themselves are changes to the method behaviour, this ensures that the test developer actually understands the context in which these functions must function appropriatley in. 

Advantages: 
- The biggest advantage to mutation testing is its ability to be automized, the test developer does not have to manually change the source code to ensure the strenght of their test suites.
- Mutation testing is most effective at the unit testing level, this includes both the design and evaluation of test cases. This is due to its focus on individual behaviours for each test and their corresponding methods and classes.
- Reporting both evaluation metrics and the mutation process is increadibly useful to test developers. They provide targets for developers to focus on, and the percentage calculations provided also give testing thresholds for people to decide on where they want to exit the testing phase. 
- This testing strategy is rather methotical, is does well with having a plan or system to follow. In which is very common in the testing phase of development. 

Disadvantages: 
- Due to its automated nature, it is very computationallly expensive. Even smaller programs, like this lab (In which we have tests for only 2 classes) it takes quite a bit of time and power to proccess. This also means this style of testing does not scale very well as a result. 
- Sometimes, mutations that are injected into code may not be indicative of actual faults with either the source code or test cases.
- It is entirely possible for equivalent mutants to lower mutation coverage due to their nature as being unkillible. In order to counteract this, developers need to understand wheree theur equivalent mutants lie. 

# Explain your SELENUIM test case design process

In order to design our tests, we initially brainstormed what interactive elements of the eBay website could be consistently recorded and tested. The final tests were based on browsing the site ourselves and jotting down relevant functionalities that could be tested, with elements that existed that could be asserted once done. At the end of the brainstorming and traversing the website, the eight tests we came up with were Add to Cart, Sign In, Remove from Cart, Add to Watchlist, Save Seller, Invalid Login, Normal Search, and Advanced Search. Creating the test cases involved using Selenium to record the test case, which was then saved into a test case within the project file and would be run several times to ensure the functionality was working as expected.

# Explain the use of assertions and checkpoints

# how did you test each functionaity with different test data

# Discuss advantages and disadvantages of Selenium vs. Sikulix

Sikulix vs Selenium
Sikuli is used for GUI test automation and identifies objects using that are displayed on the screen. It requires an image to be stored and uses an image-based recognition system to compare how much the image matches the display GUI it is searching. This can be particularly helpful for test cases such as searching on Ebay as it can also recognize the images of the search results.
A disadvantage of Selenium is that it can only be used on web applications where as Sikulix can also be used on system applications as well. Selenium also cannot be used for image recognition and is only limited to locators that can be found in the HTML source code, where as Sikulix is not limited to the HTML source code as it compares images to anything that can be seen on screen. A disadvantage of Sikulix is that it can work only with what is displayed on the GUI. Unlike Selenium, Sikulix is unable to read text and images must be stored to test automation.

# How the team work/effort was divided and managed

To start off with, all members where present when planning the work and distribution for this lab, each member was also assigned to where we thought their strengths would apply the most. Due to time constaints, we had a relativley strict schedule to follow before the demo day. Team work was divided into pairs, two working on the mutation testing using Pitest and the other took care of Selenium with the GUI testing. This did not mean that these pairs did not have any knowledge of the testing they did not do, proper steps were taken to ensure that all members understood the steps taken to complete each lab section. 

# Difficulties encountered, challenges overcome, and lessons learned

The biggest issues with this lab where the setting up of Pitest. In particular there were problems with the specific jdk used for Pitest as some versions did support it, but Pitest failed to run due to class errors, this extends to a bigger issue of sometimes there not being enough instructions to set up projects. Some if the lab instructions themselves mentioned folders that did not exist or where missing some external jar files for library purposes. It had taken some time to get the logistics working but after we did the execution went smoothly. 

In regards to the lessons leanred from this lab, despite its setbacks, this was a insighful look into the use and practice of both mutation and GUI testing. Leanring how to use these tools are very benificial, we undertsand them now in order to use in industry but students now have another way to implement quality control in larger projects if needed.

# Comments/feedback on the lab itself

Much like the previous lab, some points of the code in both Range and DataUtilities where unreachible, which in turn hurt of mutation scores. Also, the act of running the test files within the assignment artifacts was rather time and computing expensive, a smaller sample to test Pitest might have been fine. 
